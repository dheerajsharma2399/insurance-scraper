{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Document Parser - Testing Notebook\n",
    "\n",
    "This notebook tests the insurance document parser on sample PDF files.\n",
    "\n",
    "## Objectives\n",
    "1. Load and test the parser on sample documents\n",
    "2. Examine extracted fields and confidence scores\n",
    "3. Identify patterns that work well and areas for improvement\n",
    "4. Document edge cases and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path to import parser\n",
    "sys.path.append('..')\n",
    "\n",
    "from insurance_parser import InsuranceDocumentParser, ExtractionResult\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parser\n",
    "parser = InsuranceDocumentParser()\n",
    "print(\"✓ Parser initialized\")\n",
    "\n",
    "# Define paths\n",
    "SAMPLE_DOCS_DIR = Path('../sample_documents')\n",
    "RESULTS_DIR = Path('../test_results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Sample documents directory: {SAMPLE_DOCS_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. List Available Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all PDF files in sample_documents directory\n",
    "pdf_files = list(SAMPLE_DOCS_DIR.rglob('*.pdf'))\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF files:\\n\")\n",
    "for i, pdf_file in enumerate(pdf_files, 1):\n",
    "    print(f\"{i}. {pdf_file.name} ({pdf_file.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "if len(pdf_files) == 0:\n",
    "    print(\"\\n⚠️ No PDF files found. Please add sample insurance documents to the 'sample_documents' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Single Document Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a document to test (change index as needed)\n",
    "if len(pdf_files) > 0:\n",
    "    test_doc = pdf_files[0]  # Change index to test different documents\n",
    "    print(f\"Testing document: {test_doc.name}\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Parse the document\n",
    "    result = parser.parse_pdf(str(test_doc))\n",
    "    \n",
    "    # Display metadata\n",
    "    print(\"\\nDOCUMENT METADATA\")\n",
    "    print(\"=\" * 70)\n",
    "    for key, value in result.document_metadata.items():\n",
    "        print(f\"{key:20s}: {value}\")\n",
    "    \n",
    "    # Display extracted fields\n",
    "    print(\"\\n\\nEXTRACTED FIELDS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total fields extracted: {len(result.fields)}\\n\")\n",
    "    \n",
    "    for field_name, field_data in sorted(result.fields.items()):\n",
    "        print(f\"\\n{field_name.upper().replace('_', ' ')}:\")\n",
    "        print(f\"  Value:      {field_data.value}\")\n",
    "        print(f\"  Confidence: {field_data.confidence:.2%}\")\n",
    "        print(f\"  Page:       {field_data.page}\")\n",
    "        print(f\"  Context:    {field_data.context[:80]}...\")\n",
    "    \n",
    "    # Display tables\n",
    "    if result.tables_extracted:\n",
    "        print(\"\\n\\nEXTRACTED TABLES\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Total tables: {len(result.tables_extracted)}\\n\")\n",
    "        for i, table in enumerate(result.tables_extracted, 1):\n",
    "            print(f\"Table {i}:\")\n",
    "            print(f\"  Type: {table['table_type']}\")\n",
    "            print(f\"  Page: {table['page']}\")\n",
    "            print(f\"  Rows: {len(table['rows'])}\")\n",
    "    \n",
    "    # Display warnings\n",
    "    if result.warnings:\n",
    "        print(\"\\n\\nWARNINGS\")\n",
    "        print(\"=\" * 70)\n",
    "        for warning in result.warnings:\n",
    "            print(f\"⚠️  {warning}\")\n",
    "    \n",
    "    # Save results\n",
    "    output_file = RESULTS_DIR / f\"{test_doc.stem}_results.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(result.to_json())\n",
    "    print(f\"\\n\\n✓ Results saved to: {output_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No documents available to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Process All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all documents and collect results\n",
    "all_results = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"Processing: {pdf_file.name}...\")\n",
    "    try:\n",
    "        result = parser.parse_pdf(str(pdf_file))\n",
    "        all_results.append({\n",
    "            'filename': pdf_file.name,\n",
    "            'result': result,\n",
    "            'fields_count': len(result.fields),\n",
    "            'document_type': result.document_metadata['document_type'],\n",
    "            'pages': result.document_metadata['pages']\n",
    "        })\n",
    "        print(f\"  ✓ Extracted {len(result.fields)} fields\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        all_results.append({\n",
    "            'filename': pdf_file.name,\n",
    "            'result': None,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "print(f\"\\n✓ Processed {len(all_results)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for res in all_results:\n",
    "    if res.get('result'):\n",
    "        summary_data.append({\n",
    "            'Document': res['filename'],\n",
    "            'Type': res['document_type'],\n",
    "            'Pages': res['pages'],\n",
    "            'Fields Extracted': res['fields_count'],\n",
    "            'Avg Confidence': sum(f.confidence for f in res['result'].fields.values()) / len(res['result'].fields) if res['result'].fields else 0\n",
    "        })\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\nSUMMARY STATISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    print(summary_df.describe())\n",
    "else:\n",
    "    print(\"No successful extractions to summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Confidence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all confidence scores\n",
    "confidence_data = []\n",
    "for res in all_results:\n",
    "    if res.get('result'):\n",
    "        for field_name, field_data in res['result'].fields.items():\n",
    "            confidence_data.append({\n",
    "                'Field': field_name,\n",
    "                'Confidence': field_data.confidence,\n",
    "                'Document': res['filename']\n",
    "            })\n",
    "\n",
    "if confidence_data:\n",
    "    conf_df = pd.DataFrame(confidence_data)\n",
    "    \n",
    "    # Plot confidence distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(conf_df['Confidence'], bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_xlabel('Confidence Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Distribution of Confidence Scores')\n",
    "    axes[0].axvline(0.6, color='red', linestyle='--', label='Threshold (0.6)')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Box plot by field type\n",
    "    field_counts = conf_df['Field'].value_counts()\n",
    "    top_fields = field_counts.head(10).index\n",
    "    conf_df_top = conf_df[conf_df['Field'].isin(top_fields)]\n",
    "    \n",
    "    conf_df_top.boxplot(column='Confidence', by='Field', ax=axes[1], rot=45)\n",
    "    axes[1].set_title('Confidence by Field Type (Top 10)')\n",
    "    axes[1].set_xlabel('Field')\n",
    "    axes[1].set_ylabel('Confidence Score')\n",
    "    plt.suptitle('')  # Remove default title\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'confidence_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Visualization saved to {RESULTS_DIR / 'confidence_analysis.png'}\")\n",
    "else:\n",
    "    print(\"No confidence data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Field Extraction Success Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how often each field type is extracted\n",
    "if confidence_data:\n",
    "    field_counts = conf_df['Field'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    field_counts.plot(kind='barh', color='steelblue')\n",
    "    plt.xlabel('Number of Documents')\n",
    "    plt.ylabel('Field Type')\n",
    "    plt.title('Field Extraction Frequency Across All Documents')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'field_frequency.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFIELD EXTRACTION FREQUENCY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(field_counts.to_string())\n",
    "else:\n",
    "    print(\"No field data to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Low Confidence Fields Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify fields with low confidence scores\n",
    "if confidence_data:\n",
    "    low_confidence_threshold = 0.6\n",
    "    low_conf_df = conf_df[conf_df['Confidence'] < low_confidence_threshold]\n",
    "    \n",
    "    print(f\"\\nLOW CONFIDENCE FIELDS (< {low_confidence_threshold})\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total: {len(low_conf_df)} out of {len(conf_df)} ({len(low_conf_df)/len(conf_df)*100:.1f}%)\\n\")\n",
    "    \n",
    "    if len(low_conf_df) > 0:\n",
    "        print(low_conf_df.sort_values('Confidence').to_string(index=False))\n",
    "        \n",
    "        # Recommendations\n",
    "        print(\"\\n\\nRECOMMENDATIONS:\")\n",
    "        print(\"-\" * 70)\n",
    "        for field in low_conf_df['Field'].unique():\n",
    "            print(f\"• Review context keywords for '{field}'\")\n",
    "    else:\n",
    "        print(\"✓ All fields have acceptable confidence scores!\")\n",
    "else:\n",
    "    print(\"No confidence data to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary report\n",
    "report = {\n",
    "    'test_date': datetime.now().isoformat(),\n",
    "    'total_documents': len(pdf_files),\n",
    "    'successful_extractions': len([r for r in all_results if r.get('result')]),\n",
    "    'failed_extractions': len([r for r in all_results if not r.get('result')]),\n",
    "    'total_fields_extracted': sum(r.get('fields_count', 0) for r in all_results),\n",
    "    'average_confidence': conf_df['Confidence'].mean() if confidence_data else 0,\n",
    "    'low_confidence_count': len(low_conf_df) if confidence_data else 0,\n",
    "    'field_types_found': list(field_counts.index) if confidence_data else [],\n",
    "    'document_types': [r['document_type'] for r in all_results if r.get('result')]\n",
    "}\n",
    "\n",
    "report_file = RESULTS_DIR / f\"test_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(report_file, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\nTEST SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "print(json.dumps(report, indent=2))\n",
    "print(f\"\\n✓ Report saved to: {report_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "\n",
    "Based on the test results:\n",
    "\n",
    "1. **Review low confidence fields** - Check if context keywords need improvement\n",
    "2. **Examine failed extractions** - Identify patterns in documents that failed\n",
    "3. **Test edge cases** - Try scanned PDFs, poor quality documents, unusual formats\n",
    "4. **Refine patterns** - Update regex patterns in `insurance_parser.py` if needed\n",
    "5. **Add new field types** - If you find fields not currently extracted\n",
    "\n",
    "**Proceed to:**\n",
    "- `02_results_visualization.ipynb` for detailed visualizations\n",
    "- `03_pattern_refinement.ipynb` for pattern optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
